{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5e0c27b",
   "metadata": {},
   "source": [
    "## STEP 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2545b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09e9ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df = pd.read_csv('./learn-ai-bbc/BBC News Train.csv')\n",
    "\n",
    "# remove unnecessary columns\n",
    "df = df.drop(columns=[\"ArticleId\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63c7bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "    \n",
    "    # stem the words\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # join the words back into a string\n",
    "    text = \" \".join(words)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# apply the clean_text function to the Text column\n",
    "df[\"Text\"] = df[\"Text\"].apply(clean_text)\n",
    "\n",
    "# get the unique categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "308cbe73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anike\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\anike\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "categories = df[\"Category\"].unique()\n",
    "\n",
    "# define a function to calculate the TF-ICF score\n",
    "def tf_icf(term, category):\n",
    "    # calculate term frequency\n",
    "    tf = df[df[\"Category\"] == category][\"Text\"].str.count(term).sum()\n",
    "    \n",
    "    # calculate class frequency\n",
    "    cf = df[\"Text\"].str.contains(term).sum()\n",
    "    \n",
    "    # calculate inverse-class frequency\n",
    "    icf = np.log10(len(categories) / cf)\n",
    "    \n",
    "    # calculate TF-ICF score\n",
    "    tf_icf_score = tf * icf\n",
    "    \n",
    "    return tf_icf_score\n",
    "\n",
    "# create a TF-IDF vectorizer object with the TF-ICF weighting scheme\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words(\"english\"), norm=None, use_idf=False)\n",
    "\n",
    "# calculate the TF-ICF scores for each term in each category\n",
    "tf_icf_scores = {}\n",
    "for category in categories:\n",
    "    # get the documents in the current category\n",
    "    docs = df[df[\"Category\"] == category][\"Text\"]\n",
    "    \n",
    "    # fit the TF-IDF vectorizer on the documents\n",
    "    tfidf_vectorizer.fit(docs)\n",
    "    \n",
    "    # get the vocabulary and term frequencies from the vectorizer\n",
    "    vocabulary = tfidf_vectorizer.vocabulary_\n",
    "    term_freqs = tfidf_vectorizer.transform(docs).sum(axis=0)\n",
    "    \n",
    "    # calculate the TF-ICF scores for each term in the vocabulary\n",
    "    for term, index in vocabulary.items():\n",
    "        tf_icf_score = tf_icf(term, category)\n",
    "        if term not in tf_icf_scores:\n",
    "            tf_icf_scores[term] = np.zeros(len(categories))\n",
    "        tf_icf_scores[term][np.where(categories == category)[0][0]] = tf_icf_score\n",
    "    \n",
    "# transform the documents into TF-ICF vectors\n",
    "tf_icf_vectorizer = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words(\"english\"), vocabulary=tfidf_vectorizer.vocabulary_, use_idf=False, norm=\"l2\")\n",
    "tf_icf_vectors = tf_icf_vectorizer.fit_transform(df[\"Text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73f5b25",
   "metadata": {},
   "source": [
    "## STEP 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89d95702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(tf_icf_vectors, list(df['Category']), test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c552da6a",
   "metadata": {},
   "source": [
    "## STEP 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23fa4a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.92      0.98      0.95       108\n",
      "entertainment       1.00      0.84      0.91        79\n",
      "     politics       0.93      0.93      0.93        86\n",
      "        sport       0.94      1.00      0.97       101\n",
      "         tech       0.93      0.92      0.92        73\n",
      "\n",
      "     accuracy                           0.94       447\n",
      "    macro avg       0.94      0.93      0.94       447\n",
      " weighted avg       0.94      0.94      0.94       447\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_pred = list(clf.predict(X_test))\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
